{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep chip order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/home/pleung/projects/peptide_binders/r0/peptide_binders\n",
      "dig66\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "# python internal\n",
    "import collections\n",
    "import copy\n",
    "import gc\n",
    "from glob import glob\n",
    "import h5py\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import socket\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# conda/pip\n",
    "import dask\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# special packages on the DIGS\n",
    "import py3Dmol\n",
    "import pymol\n",
    "import pyrosetta\n",
    "\n",
    "# notebook magic\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(os.getcwd())\n",
    "print(socket.gethostname())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load selected designs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_json = os.path.join(os.getcwd(), \"04_run_af2_short\", \"to_order.json\")\n",
    "to_order_df = pd.read_json(input_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpools = {}\n",
    "i = 0\n",
    "for length in set(to_order_df.chA_length.values):\n",
    "    subset = to_order_df[to_order_df[\"chA_length\"] == length]\n",
    "    if len(subset) < 1000:\n",
    "        subpools[i] = subset\n",
    "        i += 1\n",
    "    else:\n",
    "        for split in np.array_split(subset, int(len(subset) / 1000) + 1):\n",
    "            subpools[i] = split\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 494\n",
      "1 907\n",
      "2 907\n",
      "3 907\n",
      "4 907\n",
      "5 907\n",
      "6 907\n",
      "7 907\n",
      "8 907\n",
      "9 907\n",
      "10 782\n",
      "11 781\n",
      "12 781\n",
      "13 946\n",
      "14 946\n",
      "15 945\n",
      "16 945\n",
      "17 945\n",
      "18 945\n",
      "19 945\n"
     ]
    }
   ],
   "source": [
    "for key, value in subpools.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make functions to domesticate sequences uses Ryan's `domesticator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row2dna(row) -> str:\n",
    "    \n",
    "    def capture_1shot_domesticator(cmd: str) -> str:\n",
    "        \"\"\"\n",
    "        run domesticator cmd. \n",
    "        split stdout into lines.\n",
    "        loop once through discarding lines up to ones including >.\n",
    "        return joined output\n",
    "        \"\"\"\n",
    "\n",
    "        def cmd_no_stderr(command, wait=True):\n",
    "            \"\"\"@nrbennet @bcov @pleung\"\"\"\n",
    "            the_command = subprocess.Popen(\n",
    "                command,\n",
    "                shell=True,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE,\n",
    "                universal_newlines=True,\n",
    "            )\n",
    "            if not wait:\n",
    "                return\n",
    "            the_stuff = the_command.communicate()\n",
    "            return str(the_stuff[0])\n",
    "\n",
    "        stdout = cmd_no_stderr(cmd)\n",
    "\n",
    "        sequence = []\n",
    "        append = False\n",
    "        for line in stdout.splitlines():\n",
    "            if append:\n",
    "                sequence.append(line)\n",
    "            else:\n",
    "                pass\n",
    "            if \">unknown_seq1\" in line:\n",
    "                append = True\n",
    "            else:\n",
    "                pass\n",
    "        to_return = \"\".join(sequence)\n",
    "        return to_return\n",
    "    \n",
    "    protein = row[\"chA_seq\"]\n",
    "    dna = capture_1shot_domesticator(f\"./domesticator.py {protein} --avoid_restriction_sites BsaI XhoI NdeI --avoid_patterns AGGAGG GCTGGTGG ATCTGTT GGRGGT GGATCC GCTAGC AAAAAAAA GGGGG TTTTTTTT CCCCCCCC CACCTGC --avoid_kmers 8 --avoid_kmers_boost 10 --species s_cerevisiae\")\n",
    "    return dna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse translate designs to order\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from glob import glob\n",
    "import logging\n",
    "import pwd\n",
    "\n",
    "print(\"run the following from your local terminal:\")\n",
    "print(\n",
    "    f\"ssh -L 8000:localhost:8787 {pwd.getpwuid(os.getuid()).pw_name}@{socket.gethostname()}\"\n",
    ")\n",
    "\n",
    "subpool_futures = {}\n",
    "translated_subpools = {}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # configure SLURM cluster as a context manager\n",
    "    with SLURMCluster(\n",
    "        cores=1,\n",
    "        processes=1,\n",
    "        job_cpu=1,\n",
    "        memory=\"1GB\",\n",
    "        queue=\"long\",\n",
    "        walltime=\"23:30:00\",\n",
    "        death_timeout=120,\n",
    "        local_directory=\"$TMPDIR/dask\",\n",
    "        log_directory=\"/mnt/home/pleung/logs/slurm_logs\",\n",
    "        extra=[\"--lifetime\", \"23h\", \"--lifetime-stagger\", \"4m\"],\n",
    "    ) as cluster:\n",
    "        print(cluster.job_script())\n",
    "        # scale between 1-50 workers,\n",
    "        cluster.adapt(\n",
    "            minimum=1,\n",
    "            maximum=20,\n",
    "            wait_count=400,  # Number of consecutive times that a worker should be suggested for removal it is removed\n",
    "            interval=\"5s\",  # Time between checks\n",
    "        )\n",
    "        # setup a client to interact with the cluster as a context manager\n",
    "        with Client(cluster) as client:\n",
    "            print(client)\n",
    "            for subpool, df in subpools.items():\n",
    "                future_df = client.submit(df.apply, row2dna, axis=1)\n",
    "                subpool_futures[subpool] = future_df\n",
    "            for subpool, pending in subpool_futures.items():\n",
    "                translated_subpools[subpool] = pending.result()\n",
    "            output_dir = os.path.join(os.getcwd(), \"05_to_order\")\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            with open(os.path.join(output_dir, \"subpools.pkl\"), \"wb\") as handle:\n",
    "                pickle.dump(translated_subpools, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108it [08:44,  4.85s/it]\n",
      "12it [01:00,  5.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# prefix = \"atactacggtctcaagga\"  # for scarless cloning\n",
    "# irbs = \"AGCGGCGGCAGCTAGTAAAGAAGGAGATATCATATGAGCGGCGGCAGC\"  # for bicstronic expression and normalization\n",
    "# suffix = \"GGttcccgagaccgtaatgc\"  # for scarless cloning\n",
    "# with open(os.path.join(os.getcwd(), \"07_to_order\", \"cs_dna.list\"), \"w+\") as bicis:\n",
    "#     for index, row in tqdm(to_order_df.iterrows()):\n",
    "#         chA = row[\"chA_seq\"]\n",
    "#         chB = row[\"chB_seq\"]\n",
    "#         len_chB_dna = 3 * len(chB)\n",
    "#         fully_domesticated = capture_1shot_domesticator(\n",
    "#             cmd_no_stderr(\n",
    "#                 f\"./domesticator.py {chB+chA} --avoid_restriction_sites BsaI XhoI NdeI --avoid_patterns AGGAGG GCTGGTGG ATCTGTT GGRGGT --avoid_kmers 8 --avoid_kmers_boost 10 --species e_coli\"\n",
    "#             )\n",
    "#         )\n",
    "#         chB_dna = fully_domesticated[:len_chB_dna]\n",
    "#         chA_dna = fully_domesticated[len_chB_dna:]\n",
    "#         bicis.write(index + \"\\t\" + prefix + chB_dna + irbs + chA_dna + suffix + \"\\n\")\n",
    "# with open(os.path.join(os.getcwd(), \"07_to_order\", \"jhb_dna.list\"), \"w+\") as bicis:\n",
    "#     for index, row in tqdm(to_order_2.iterrows()):\n",
    "#         chA = row[\"chA_seq\"]\n",
    "#         chB = row[\"chB_seq\"]\n",
    "#         len_chB_dna = 3 * len(chB)\n",
    "#         fully_domesticated = capture_1shot_domesticator(\n",
    "#             cmd_no_stderr(\n",
    "#                 f\"./domesticator.py {chB+chA} --avoid_restriction_sites BsaI XhoI NdeI --avoid_patterns AGGAGG GCTGGTGG ATCTGTT GGRGGT --avoid_kmers 8 --avoid_kmers_boost 10 --species e_coli\"\n",
    "#             )\n",
    "#         )\n",
    "#         chB_dna = fully_domesticated[:len_chB_dna]\n",
    "#         chA_dna = fully_domesticated[len_chB_dna:]\n",
    "#         bicis.write(index + \"\\t\" + prefix + chB_dna + irbs + chA_dna + suffix + \"\\n\")\n",
    "\n",
    "for pool, df in subpools.items():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, subpool in subpools.items():\n",
    "    with open(f\"test_{i}.fasta\", \"w\") as f:\n",
    "        for j, row in subpool.iterrows():\n",
    "            print(f\">{j}\", file=f)\n",
    "            print(f\"{row['chA_seq']}\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "\n",
    "def run_af2(\n",
    "    prefix=\"\",  # prefix for saving pdbs, can include path components\n",
    "    query=\"\",  # relative or abspath to pdb, pdb.gz, or pdb.bz2\n",
    "    num_recycle=3,  # set this to 10 if plddts are low - might help models converge\n",
    "    random_seed=0,  # try changing seed if you need to sample more\n",
    "    num_models=5,  # it will run [4, 3, 5, 2, 1][:num_models] these models, 4 is used for compiling jax params\n",
    "    index_gap=200,  # decrease under 32 if you have a prior that chains need to interact\n",
    "    save_pdbs=True,  # if false will save pdbstring in output dict instead\n",
    ") -> Dict:\n",
    "    import bz2\n",
    "    import os\n",
    "\n",
    "    os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "    os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "    from string import ascii_uppercase\n",
    "    import sys\n",
    "\n",
    "    sys.path.insert(0, \"/projects/ml/alphafold/alphafold_git/\")\n",
    "    from typing import Dict\n",
    "    import jax\n",
    "    from jax.lib import xla_bridge\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    import numpy as np\n",
    "    from alphafold.common import protein\n",
    "    from alphafold.data import pipeline\n",
    "    from alphafold.data import templates\n",
    "    from alphafold.model import data\n",
    "    from alphafold.model import config\n",
    "    from alphafold.model import model\n",
    "    from alphafold.relax import relax\n",
    "    from alphafold.relax import utils\n",
    "    import pyrosetta\n",
    "    import pyrosetta.distributed.io as io\n",
    "    from pyrosetta.distributed.tasks.rosetta_scripts import (\n",
    "        SingleoutputRosettaScriptsTask,\n",
    "    )\n",
    "    from pyrosetta.rosetta.core.pose import Pose\n",
    "\n",
    "    def set_bfactor(pose: Pose, lddt_array: list) -> None:\n",
    "        for resid, residue in enumerate(pose.residues, start=1):\n",
    "            for i, atom in enumerate(residue.atoms(), start=1):\n",
    "                pose.pdb_info().bfactor(resid, i, lddt_array[resid - 1])\n",
    "        return\n",
    "\n",
    "    def mk_mock_template(query_sequence: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Make a mock template dict from a query sequence.\n",
    "        Since alphafold's model requires a template input,\n",
    "        we create a blank example w/ zero input, confidence -1\n",
    "        @minkbaek @aivan\n",
    "        \"\"\"\n",
    "        ln = len(query_sequence)\n",
    "        output_templates_sequence = \"-\" * ln\n",
    "        output_confidence_scores = np.full(ln, -1)\n",
    "        templates_all_atom_positions = np.zeros(\n",
    "            (ln, templates.residue_constants.atom_type_num, 3)\n",
    "        )\n",
    "        templates_all_atom_masks = np.zeros(\n",
    "            (ln, templates.residue_constants.atom_type_num)\n",
    "        )\n",
    "        templates_aatype = templates.residue_constants.sequence_to_onehot(\n",
    "            output_templates_sequence, templates.residue_constants.HHBLITS_AA_TO_ID\n",
    "        )\n",
    "        template_features = {\n",
    "            \"template_all_atom_positions\": templates_all_atom_positions[None],\n",
    "            \"template_all_atom_masks\": templates_all_atom_masks[None],\n",
    "            \"template_sequence\": [f\"none\".encode()],\n",
    "            \"template_aatype\": np.array(templates_aatype)[None],\n",
    "            \"template_confidence_scores\": output_confidence_scores[None],\n",
    "            \"template_domain_names\": [f\"none\".encode()],\n",
    "            \"template_release_date\": [f\"none\".encode()],\n",
    "        }\n",
    "        return template_features\n",
    "\n",
    "    def get_rmsd(design: Pose, prediction: Pose) -> float:\n",
    "        \"\"\"Calculate Ca-RMSD of prediction to design\"\"\"\n",
    "        rmsd_calc = pyrosetta.rosetta.core.simple_metrics.metrics.RMSDMetric()\n",
    "        rmsd_calc.set_rmsd_type(pyrosetta.rosetta.core.scoring.rmsd_atoms(3))\n",
    "        rmsd_calc.set_run_superimpose(True)\n",
    "        rmsd_calc.set_comparison_pose(design)\n",
    "        rmsd = float(rmsd_calc.calculate(prediction))\n",
    "        return rmsd\n",
    "\n",
    "    def DAN(pdb: str) -> np.array:\n",
    "        import os, subprocess\n",
    "\n",
    "        def cmd(command, wait=True):\n",
    "            \"\"\"@nrbennet @bcov\"\"\"\n",
    "            the_command = subprocess.Popen(\n",
    "                command,\n",
    "                shell=True,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE,\n",
    "                universal_newlines=True,\n",
    "            )\n",
    "            if not wait:\n",
    "                return\n",
    "            the_stuff = the_command.communicate()\n",
    "            return str(the_stuff[0]) + str(the_stuff[1])\n",
    "\n",
    "        pythonpath = \"/software/conda/envs/tensorflow/bin/python\"\n",
    "        script = \"/net/software/DeepAccNet/DeepAccNet.py\"\n",
    "        npz = pdb.replace(\".pdb\", \".npz\")\n",
    "        to_send = f\"\"\"{pythonpath} {script} -r -v --pdb {pdb} {npz} \"\"\"\n",
    "        print(cmd(to_send))\n",
    "        x = np.load(npz)\n",
    "        os.remove(npz)\n",
    "        lddt = x[\"lddt\"]\n",
    "        return lddt\n",
    "\n",
    "    def predict_structure(\n",
    "        prefix=\"\",\n",
    "        feature_dict={},\n",
    "        Ls=[],\n",
    "        model_params={},\n",
    "        use_model={},\n",
    "        random_seed=0,\n",
    "        index_gap=200,\n",
    "        save_pdbs=True,\n",
    "    ) -> Dict:\n",
    "        \"\"\"Predicts structure using AlphaFold for the given pdb/pdb.bz2.\"\"\"\n",
    "        # Minkyung\"s code adds big enough number to residue index to indicate chain breaks\n",
    "        idx_res = feature_dict[\"residue_index\"]\n",
    "        L_prev = 0\n",
    "        # Ls: number of residues in each chain\n",
    "        for L_i in Ls[:-1]:\n",
    "            idx_res[L_prev + L_i :] += index_gap\n",
    "            L_prev += L_i\n",
    "        feature_dict[\"residue_index\"] = idx_res\n",
    "        # Run the models.\n",
    "        plddts, paes, ptms, rmsds = [], [], [], []\n",
    "        poses = []\n",
    "        for model_name, params in model_params.items():\n",
    "            if model_name in use_model:\n",
    "                model_runner = model_runner_4  # global, only compile once\n",
    "                model_runner.params = params\n",
    "                processed_feature_dict = model_runner.process_features(\n",
    "                    feature_dict, random_seed=random_seed\n",
    "                )\n",
    "                prediction_result = model_runner.predict(processed_feature_dict)\n",
    "                unrelaxed_protein = protein.from_prediction(\n",
    "                    processed_feature_dict, prediction_result\n",
    "                )\n",
    "                plddts.append(prediction_result[\"plddt\"])\n",
    "                paes.append(prediction_result[\"predicted_aligned_error\"])\n",
    "                ptms.append(prediction_result[\"ptm\"])\n",
    "                # add termini after each chain\n",
    "                unsafe_pose = io.to_pose(\n",
    "                    io.pose_from_pdbstring(protein.to_pdb(unrelaxed_protein))\n",
    "                )\n",
    "                cleaned_pose = Pose()\n",
    "                total = 0\n",
    "                chunks = []\n",
    "                mylist = list(unsafe_pose.residues)\n",
    "                for j in range(len(Ls)):\n",
    "                    chunk_mylist = mylist[total : total + Ls[j]]\n",
    "                    chunks.append(chunk_mylist)\n",
    "                    total += Ls[j]\n",
    "                    temp_pose = Pose()\n",
    "                    for k in chunk_mylist:\n",
    "                        temp_pose.append_residue_by_bond(k)\n",
    "                    pyrosetta.rosetta.core.pose.append_pose_to_pose(\n",
    "                        cleaned_pose, temp_pose, True\n",
    "                    )\n",
    "                sc = pyrosetta.rosetta.protocols.simple_moves.SwitchChainOrderMover()\n",
    "                sc.chain_order(\"\".join([str(i) for i in range(1, len(Ls) + 1)]))\n",
    "                sc.apply(cleaned_pose)\n",
    "                rmsds.append(get_rmsd(pose, cleaned_pose))\n",
    "                # relax sidechains to prevent distracting clashes in output\n",
    "                xml = \"\"\"\n",
    "                <ROSETTASCRIPTS>\n",
    "                    <SCOREFXNS>\n",
    "                        <ScoreFunction name=\"sfxn\" weights=\"beta_nov16\" />\n",
    "                    </SCOREFXNS>\n",
    "                    <RESIDUE_SELECTORS>\n",
    "                    </RESIDUE_SELECTORS>\n",
    "                    <TASKOPERATIONS>\n",
    "                    </TASKOPERATIONS>\n",
    "                    <TASKOPERATIONS>\n",
    "                        <IncludeCurrent name=\"current\" />\n",
    "                    </TASKOPERATIONS>\n",
    "                    <MOVERS>\n",
    "                        <FastRelax name=\"relax\" scorefxn=\"sfxn\" repeats=\"1\" bondangle=\"false\" bondlength=\"false\" task_operations=\"current\" >\n",
    "                            <MoveMap name=\"MM\" bb=\"false\" chi=\"true\" jump=\"false\" />\n",
    "                        </FastRelax>\n",
    "                    </MOVERS>\n",
    "                    <FILTERS>\n",
    "                    </FILTERS>\n",
    "                    <APPLY_TO_POSE>\n",
    "                    </APPLY_TO_POSE>\n",
    "                    <PROTOCOLS>\n",
    "                        <Add mover=\"relax\" />\n",
    "                    </PROTOCOLS>\n",
    "                    <OUTPUT />\n",
    "                </ROSETTASCRIPTS>\n",
    "                \"\"\"\n",
    "                relaxer = SingleoutputRosettaScriptsTask(xml)\n",
    "                relaxed_ppose = relaxer(cleaned_pose.clone())\n",
    "                poses.append(io.to_pose(relaxed_ppose))\n",
    "                # cleanup some memory\n",
    "                del processed_feature_dict, prediction_result\n",
    "\n",
    "        model_idx = [4, 3, 5, 1, 2]\n",
    "        model_idx = model_idx[:num_models]\n",
    "        out = {}\n",
    "        # save output pdbs and metadata\n",
    "        for n, r in enumerate(model_idx):\n",
    "            os.makedirs(\n",
    "                os.path.join(os.getcwd(), \"/\".join(prefix.split(\"/\")[:-1])),\n",
    "                exist_ok=True,\n",
    "            )\n",
    "            relaxed_pdb_path = f\"{prefix}_relaxed_model_{r}.pdb\"\n",
    "            set_bfactor(poses[n], list(plddts[n]))\n",
    "            poses[n].dump_pdb(relaxed_pdb_path)\n",
    "            average_plddts = float(plddts[n].mean())\n",
    "\n",
    "            out[f\"model_{r}\"] = {\n",
    "                \"average_plddts\": average_plddts,\n",
    "                \"plddt\": plddts[n].tolist(),\n",
    "                \"pae\": paes[n].tolist(),\n",
    "                \"ptm\": ptms[n].tolist(),\n",
    "                \"rmsd_to_input\": rmsds[n],\n",
    "                \"pdb_path\": os.path.abspath(relaxed_pdb_path),\n",
    "            }\n",
    "            print(f\"model_{r}: average plddt {average_plddts}\")\n",
    "        return out\n",
    "\n",
    "    # begin main method\n",
    "    pyrosetta.init(\"-run:constant_seed 1 -mute all -corrections::beta_nov16 true\")\n",
    "    # read in pdbs, do bz2 check, if query does not contain .pdb throw exception\n",
    "    if \".pdb\" in query and \".bz2\" not in query:\n",
    "        pose = pyrosetta.io.pose_from_file(query)\n",
    "    elif \".pdb.bz2\" in query:\n",
    "        with open(query, \"rb\") as f:\n",
    "            ppose = io.pose_from_pdbstring(bz2.decompress(f.read()).decode())\n",
    "        pose = io.to_pose(ppose)\n",
    "    else:\n",
    "        raise RuntimeError(\"query must be a pdb, pdb.gz, or pdb.bz2\")\n",
    "    n_chains = pose.num_chains()\n",
    "    seqs = [chain.sequence() for chain in pose.split_by_chain()]\n",
    "    full_sequence = \"\".join(seqs)\n",
    "    # prepare models\n",
    "    use_model = {}\n",
    "    if \"model_params\" not in dir():\n",
    "        model_params = {}\n",
    "    for model_name in [\"model_4\", \"model_3\", \"model_5\", \"model_1\", \"model_2\"][\n",
    "        :num_models\n",
    "    ]:\n",
    "        use_model[model_name] = True\n",
    "        if model_name not in model_params:\n",
    "            model_params[model_name] = data.get_model_haiku_params(\n",
    "                model_name=model_name + \"_ptm\",\n",
    "                data_dir=\"/projects/ml/alphafold/alphafold_git/\",\n",
    "            )\n",
    "            if (\n",
    "                model_name == \"model_4\"\n",
    "            ):  # compile only model 4 and later load weights for other models\n",
    "                model_config = config.model_config(model_name + \"_ptm\")\n",
    "                model_config.data.common.max_extra_msa = 1\n",
    "                model_config.data.eval.max_msa_clusters = n_chains\n",
    "                model_config.data.eval.num_ensemble = 1\n",
    "                model_config.data.common.num_recycle = num_recycle\n",
    "                model_runner_4 = model.RunModel(model_config, model_params[model_name])\n",
    "    # prepare input data\n",
    "    template_features = mk_mock_template(full_sequence)  # make mock template\n",
    "    deletion_matrix = [[0] * len(full_sequence)]  # make mock deletion matrix\n",
    "    msas = []\n",
    "    deletion_matrices = []\n",
    "    for i in range(n_chains):\n",
    "        # make a sequence of length full_sequence where everything but the i-th chain is \"-\"\n",
    "        msa = [\n",
    "            \"\".join([\"-\" * len(seq) if i != j else seq for j, seq in enumerate(seqs)])\n",
    "        ]\n",
    "        msas.append(msa)\n",
    "        deletion_matrices.append(deletion_matrix)\n",
    "    feature_dict = {\n",
    "        **pipeline.make_sequence_features(\n",
    "            sequence=full_sequence,\n",
    "            description=\"none\",\n",
    "            num_res=len(full_sequence),\n",
    "        ),\n",
    "        **pipeline.make_msa_features(msas=msas, deletion_matrices=deletion_matrices),\n",
    "        **template_features,\n",
    "    }\n",
    "    # predict structure\n",
    "    if prefix == \"\":\n",
    "        prefix = query\n",
    "    else:\n",
    "        pass\n",
    "    out = predict_structure(\n",
    "        prefix=prefix,\n",
    "        feature_dict=feature_dict,\n",
    "        Ls=[len(l) for l in seqs],\n",
    "        model_params=model_params,\n",
    "        use_model=use_model,\n",
    "        random_seed=random_seed,\n",
    "        index_gap=index_gap,\n",
    "        save_pdbs=save_pdbs,\n",
    "    )\n",
    "    # deallocate backend memory to make room for DAN\n",
    "    # TODO delete runners/config?\n",
    "    del model_params\n",
    "    device = xla_bridge.get_backend().platform\n",
    "    backend = xla_bridge.get_backend(device)\n",
    "    for buffer in backend.live_buffers():\n",
    "        buffer.delete()\n",
    "    # run DAN\n",
    "    for model, result in out.items():\n",
    "        pdb_path = result[\"pdb_path\"]\n",
    "        # DAN_plddt = DAN(pdb_path)\n",
    "        # result[\"average_DAN_plddts\"] = float(DAN_plddt.mean())\n",
    "        # result[\"DAN_plddt\"] = DAN_plddt.tolist()\n",
    "        # if not save, write pdbstrings to output dict\n",
    "        if not save_pdbs:\n",
    "            result[\"pdb_string\"] = io.to_pdbstring(io.pose_from_file(pdb_path))\n",
    "            os.remove(pdb_path)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup dask, set command line options, make tasks and submit to client again to cleanup disulfides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from glob import glob\n",
    "import logging\n",
    "import pwd\n",
    "from pyrosetta.distributed.cluster.core import PyRosettaCluster\n",
    "\n",
    "\n",
    "print(\"run the following from your local terminal:\")\n",
    "print(\n",
    "    f\"ssh -L 8000:localhost:8787 {pwd.getpwuid(os.getuid()).pw_name}@{socket.gethostname()}\"\n",
    ")\n",
    "\n",
    "\n",
    "def create_tasks(to_clean):\n",
    "    with open(to_clean, \"r\") as f:\n",
    "        for file in f:\n",
    "            tasks = {\"options\": \"-corrections::beta_nov16 true\"}\n",
    "            tasks[\"extra_options\"] = options\n",
    "            tasks[\"-s\"] = file.rstrip()\n",
    "            yield tasks\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "options = {\n",
    "    \"-out:level\": \"300\",\n",
    "    \"-precompute_ig\": \"true\",\n",
    "    \"-detect_disulf\": \"false\",\n",
    "    \"-holes:dalphaball\": \"/home/bcov/ppi/tutorial_build/main/source/external/DAlpahBall/DAlphaBall.gcc\",\n",
    "    \"-indexed_structure_store:fragment_store\": \"/net/databases/VALL_clustered/connect_chains/ss_grouped_vall_helix_shortLoop.h5\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup disulfides\n",
    "Do short designs first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean = os.path.join(os.getcwd(), \"03_filter/cleanup_short.list\")\n",
    "output_path = os.path.join(os.getcwd(), \"04_cleanup_short\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # configure SLURM cluster as a context manager\n",
    "    with SLURMCluster(\n",
    "        cores=1,\n",
    "        processes=1,\n",
    "        job_cpu=1,\n",
    "        memory=\"6GB\",\n",
    "        queue=\"medium\",\n",
    "        walltime=\"23:30:00\",\n",
    "        death_timeout=120,\n",
    "        local_directory=\"$TMPDIR/dask\",\n",
    "        log_directory=\"/mnt/home/pleung/logs/slurm_logs\",\n",
    "        extra=[\"--lifetime\", \"23h\", \"--lifetime-stagger\", \"4m\"],\n",
    "    ) as cluster:\n",
    "        print(cluster.job_script())\n",
    "        # scale between 1-510 workers,\n",
    "        cluster.adapt(\n",
    "            minimum=1,\n",
    "            maximum=510,\n",
    "            wait_count=400,  # Number of consecutive times that a worker should be suggested for removal it is removed\n",
    "            interval=\"5s\",  # Time between checks\n",
    "        )\n",
    "        # setup a client to interact with the cluster as a context manager\n",
    "        with Client(cluster) as client:\n",
    "            print(client)\n",
    "            PyRosettaCluster(\n",
    "                tasks=create_tasks(to_clean),\n",
    "                client=client,\n",
    "                scratch_dir=output_path,\n",
    "                output_path=output_path,\n",
    "            ).distribute(protocols=[finalize_design])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at scores\n",
    "Hacky function to load JSON-like data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_scorefile(scores):\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    dfs = []\n",
    "    with open(scores, \"r\") as f:\n",
    "        for line in tqdm(f):\n",
    "            dfs.append(pd.read_json(line).T)\n",
    "    tabulated_scores = pd.concat(dfs)\n",
    "    return tabulated_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get short first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "output_path = os.path.join(os.getcwd(), \"04_cleanup_short\")\n",
    "scores = os.path.join(output_path, \"scores.json\")\n",
    "scores_df = read_scorefile(scores)\n",
    "scores_df.to_json(os.path.join(output_path, \"scores.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.read_json(os.path.join(os.getcwd(), \"04_cleanup_short\", \"scores.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting functions and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    context=\"talk\",\n",
    "    font_scale=1.5,  # make the font larger; default is pretty small\n",
    "    style=\"ticks\",  # make the background white with black lines\n",
    "    palette=\"colorblind\",  # a color palette that is colorblind friendly!\n",
    ")\n",
    "\n",
    "\n",
    "def rho(x, y, ax=None, **kwargs):\n",
    "    \"\"\"Plot the correlation coefficient in the top left hand corner of a plot.\n",
    "    https://stackoverflow.com/questions/50832204/show-correlation-values-in-pairplot-using-seaborn-in-python/50835066\n",
    "    \"\"\"\n",
    "    import scipy\n",
    "\n",
    "    r, _ = scipy.stats.pearsonr(x, y)\n",
    "    ax = ax or plt.gca()\n",
    "    # Unicode for lowercase rho (Ï)\n",
    "    rho = \"\\u03C1\"\n",
    "    ax.annotate(f\"{rho} = {r:.2f}\", xy=(0.1, 0.9), xycoords=ax.transAxes)\n",
    "\n",
    "\n",
    "def plot_unity(xdata, ydata, **kwargs):\n",
    "    \"\"\"https://stackoverflow.com/questions/48122019/how-can-i-plot-identity-lines-on-a-seaborn-pairplot\"\"\"\n",
    "    xmin, ymin = (xdata.min(), ydata.min())\n",
    "    xmax, ymax = (xdata.max(), ydata.max())\n",
    "    xpoints = np.linspace(xmin, xmax, 100)\n",
    "    ypoints = np.linspace(ymin, ymax, 100)\n",
    "    plt.gca().plot(\n",
    "        xpoints, ypoints, color=\"k\", marker=None, linestyle=\"--\", linewidth=4.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze mlfold results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\n",
    "    os.path.join(os.getcwd(), \"04_af2_short_sample\", \"results\", \"scores.csv\")\n",
    ")\n",
    "\n",
    "\n",
    "def row2design(row) -> str:\n",
    "    prediction = (\n",
    "        \"/home/pleung/projects/peptide_binders/r0/peptide_binders/04_af2_short_sample/\"\n",
    "        + row[\"Name\"]\n",
    "        + \".pdb\"\n",
    "    )\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def row2prediction(row) -> str:\n",
    "    prediction = (\n",
    "        \"/home/pleung/projects/peptide_binders/r0/peptide_binders/04_af2_short_sample/results/\"\n",
    "        + row[\"ID\"]\n",
    "        + \"_model_4.pdb\"\n",
    "    )\n",
    "    return prediction\n",
    "\n",
    "\n",
    "results_df[\"design\"] = results_df.apply(row2design, axis=1)\n",
    "results_df[\"prediction\"] = results_df.apply(row2prediction, axis=1)\n",
    "results_df = results_df.drop([\"ID\", \"Name\"], axis=1)\n",
    "results_df.iloc[0][\"prediction\"]\n",
    "\n",
    "realrows = []\n",
    "for i, row in results_df.iterrows():\n",
    "    if row[\"Sequence\"] == \"Sequence\":\n",
    "        pass\n",
    "    else:\n",
    "        realrows.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    realrows, columns=[\"Sequence\", \"Model/Tag\", \"lDDT\", \"Time\", \"design\", \"prediction\"]\n",
    ")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check RMSD to input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyrosetta\n",
    "\n",
    "\n",
    "def get_rmsd(row) -> float:\n",
    "\n",
    "    design = row[\"design\"]\n",
    "    prediction = row[\"prediction\"]\n",
    "    rmsd_calc = pyrosetta.rosetta.core.simple_metrics.metrics.RMSDMetric()\n",
    "    rmsd_calc.set_rmsd_type(pyrosetta.rosetta.core.scoring.rmsd_atoms(3))\n",
    "    rmsd_calc.set_run_superimpose(True)\n",
    "    design_pose = pyrosetta.io.pose_from_file(design)\n",
    "    chA = design_pose.split_by_chain(1)\n",
    "    rmsd_calc.set_comparison_pose(chA)\n",
    "    prediction_pose = pyrosetta.io.pose_from_file(prediction)\n",
    "    rmsd = float(rmsd_calc.calculate(prediction_pose))\n",
    "    return rmsd\n",
    "\n",
    "\n",
    "pyrosetta.init()\n",
    "results_df[\"rmsd\"] = results_df.apply(get_rmsd, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unused blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import pyrosetta\n",
    "from pyrosetta.distributed import cluster\n",
    "import pyrosetta.distributed.io as io\n",
    "\n",
    "flags = \"\"\"\n",
    "-out:level 300\n",
    "-precompute_ig true\n",
    "-detect_disulf false\n",
    "-corrections::beta_nov16 true\n",
    "-holes:dalphaball /home/bcov/ppi/tutorial_build/main/source/external/DAlpahBall/DAlphaBall.gcc\n",
    "-indexed_structure_store:fragment_store /net/databases/VALL_clustered/connect_chains/ss_grouped_vall_helix_shortLoop.h5\n",
    "\"\"\"\n",
    "# pyrosetta.distributed.init(\" \".join(flags.replace(\"\\n\\t\", \" \").split()))\n",
    "pyrosetta.init(\" \".join(flags.replace(\"\\n\\t\", \" \").split()))\n",
    "\n",
    "t = finalize_design(\n",
    "    None,\n",
    "    **{\n",
    "        \"-s\": \"/mnt/home/pleung/projects/peptide_binders/r0/peptide_binders/03_detail_0/0081/47f6c6fcb6d1e9c788e002cfeb798d0bfbc3f514e73931c8.pdb.bz2\",\n",
    "#         \"-s\": \"/mnt/home/pleung/projects/peptide_binders/r0/peptide_binders/03_detail_0/0018/8aeb40fd33d90cbae3429aac01b14b7f45a054268e56fea7.pdb.bz2\",\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crispy (3.8.10)",
   "language": "python",
   "name": "crispy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
